# Entropy calculator and random byte generator

The goal of this lab(COEN 146) was to create a random byte generator, then test it using an Entropy calculator modeled after the algorithm defined in this article from Oracle: (http://blogs.cisco.com/security/on_information_entropy/).

*******************///////////////////////////////////////////////////////////////////////////////******************* 
A quick note on ~randomness~
Computers are notoriably terrible at producing truly random numbers. In fact, most "random" number generators are actually PRNGs or pseudo random number generators. The built in C library function rand() is probably one of the worst offenders of all. For the purposes of this lab I wrote my own random number generator. While it does still rely on the clock (yes I know...), it encorporates a bit shifting technique which I think is pretty cool. The best-case solution for achieving true randomness is to use an input from the the realm outside of the computer such as atmospheric noise or even the goo patterns of a lava lamp. Weird, right?


//** --------------------------------------RANDOM NUMBER GENERATOR-----------------------------------------------**//

I generated random numbers by first seeding a key using the C library function clock() to get the nano seconds that had passed since the programs was initiated. I then leftwise bit-shifted this number by one digit. I saved the bit shifted number to a temporary variable and then used an XOR cipher with this temporary number and the byte representation of the original recorded time. I then took the least most significant eight bits of the resultant number and used this to generate a character. These characters were then used to fill a file of unspecified size through STDout.

Example of a byte generated                      = |1|0|0|1|0|1|1|1|
generated byte bit shifted(<<)left by one digit  = |0|0|1|0|1|1|1|0|
XOR (\oplus) these numbers together              -------------------
Resultant number                                 = |1|0|1|1|1|0|0|1|


In order to test my entropy and random number generator I first generated three different files using my random number generator. One was 4 megabytes, one was 8 megabytes and one was 9 megabytes.

The entropy of the first was 7.9988073, the second was 7.998987, and the third was 7.998581.
This means that as the file size grew larger between the first and the second so did the entropy. This makes sense due to the way Entropy is calculated, which I will explore below.

//**-------------------------------------------ENTROPY DEFINITION------------------------------------------------**//

Entropy is a term one might recognize from the Second Law of Thermodynamics. However the sort of entropy that I explore in the following program is a little different in both calculation and context. Entropy, in the IT theory context, is a measurement assigned to a stoichastic or randomly derived variable. It is incredibly important in the realm of cybersecurity as it can be used to determine the quality of certain crytographic elements such as salts and keys. The main theory behind assigning a variable entropy is that the more possible outcomes {x1,x2,...,xn} that a random variable X has, the higher its uncertainty. 

//**-------------------------------------------ENTROPY CALCULATION-----------------------------------------------**//

The calculator starts by filtering characters from the input text file through stdin into a buffer array. This information is then used to fill an array of relative frequencies, i.e. each time a character is repeated, that character's represetative array value is increased by one. These frequencies are then inputted into Shannon's formula for calculating Entropy.

First the probability of each specific outcome is calculated by dividing the relative frequency by the total number of variables (in this case 256). Then comes the fun part.

We then multiply this probability(probOUTCOME) by log(1/probOUTCOME). This value is then summed up into the overall entropy and the process is repeated until the entire frequency array has been traversed. What this really means is as the number of possible outcomes grows, the total entropy likewise grows.

Ok, but what does that number even mean?
The numeric value of this variable is representative of the number of bits it takes to encode all possible outcomes of the random variable. For example a die has 6 possible sides, all equally likely. Therefore its entropy should be a little less than 3 since 2^3 = 8, whereas flipping a coin(2 outcomes, equally likely) has an entropy of 1 since 2^1 = 2.  

I also tested three different file types using this metric: an image file (resulting entropy: 7.984244), a gif(resulting entropy: 7.925690) and an mp3 file(resulting entropy: 7.821666).


*****///////////////////////////////////////////////////////////////////////////////////////////////////////////*****
